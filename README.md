# Image Captioning Web Application
This project implements a real-time Image Captioning system using a Vision Transformer (ViT) as encoder and GPT-2 as decoder. The system generates natural language captions for images captured via webcam, wrapped in a user-friendly web interface powered by Flask.
## Features
- Capture images directly from webcam and send them to the captioning model via a RESTful API.
- Transformer-based encoder-decoder architecture for high-quality caption generation.
- Pretrained weights used for both ViT and GPT-2 to enhance performance and reduce training cost.
- Real-time caption rendering on the frontend after prediction.
- TTS (Text-To-Speech) support for audio output of generated captions.
## Model Architecture
<img src="https://github.com/user-attachments/assets/38769c0a-fe4a-405d-a730-459d08a2aa6c" width="400" />

The model is a Vision-Language Transformer that consists of:

**üî∑Encoder ‚Äì Vision Transformer (ViT)**
- The image is divided into patches (e.g., 16√ó16), each of which is linearly embedded.
- A [CLS] token is prepended, and positional embeddings are added.
- The patch embeddings pass through 12 ViT Encoder Blocks, each consisting of:
  - Multi-Head Self-Attention
  - MLP (Feed Forward)
  - Layer Normalization and Residual Connections

The output [CLS] embedding acts as the visual representation for the decoder.

**üî∂ Decoder ‚Äì GPT-2 Language Model**
- The decoder is based on 12-layer GPT-2 Transformer blocks.
- The input caption tokens are embedded and added with positional encoding.
- The GPT-2 block performs:
  - Causal Multi-Head Self-Attention
  - Cross-Attention with ViT encoder output
  - MLP, Layer Normalization
 
**‚û°Ô∏è Output**
- The final output logits are generated by a linear layer (LM Head) on top of the decoder's output.
- The model is trained to generate the correct caption conditioned on the visual representation.
## Dataset
- MS COCO 2014 dataset was used for training and evaluation.
- Image-caption pairs were preprocessed using:
  - Tokenization with GPT2TokenizerFast
  - Image resizing and normalization compatible with ViT
 
## Training & Results
Trained for 10 epochs with:
- Optimizer: AdamW
- Loss: Categorical cross-entropy
- Batch size: 32
- Learning rate: 1e-4
<img src="https://github.com/user-attachments/assets/869e3605-e095-4035-b02e-1d24c8c05869" width="500" />

Loss plot


<img src="https://github.com/user-attachments/assets/e22b49c7-8da1-448e-ad11-787a066e3cb0" width="400" />
<img src="https://github.com/user-attachments/assets/04a387d4-c5f9-4b05-9cd9-ea7c91c1706a" width="390" />

## Model Demo

The image below shows how the Image Captioning model performs on a test image:

<img src="https://github.com/user-attachments/assets/c7b7dd17-d9de-469b-be5c-6cf0930e58bc" width="400" />

<img src="https://github.com/user-attachments/assets/d8cbe93b-0171-437b-8501-aac4a38de139" width="400" />


## Web Interface

Built using Flask for backend and HTML/JS for frontend.

Main features:
- Capture image from webcam
- Send image to backend API (/caption)
- Display generated caption
- Optionally speak the caption via TTS
## Run Locally
1. Clone the Repository

```bash
git clone https://github.com/wangzin10/image_captioning_app.git
cd image-captioning-app
```

2. Install Dependencies

```bash
pip install -r requirements.txt
```


3. Start the Web App

```bash
python app.py
```

4. (Optional) Expose to the Internet using ngrok

Install ngrok and expose your Flask app to the internet:
```bash
ngrok http 5000
```
## File Structure

```bash
image-captioning-app/
‚îú‚îÄ‚îÄ app.py                 # Flask backend
‚îú‚îÄ‚îÄ model_loader/          # VisionGPT2Model code
‚îú‚îÄ‚îÄ static/                # Output audio file (generated from TTS)
‚îú‚îÄ‚îÄ templates/             # HTML templates
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```
