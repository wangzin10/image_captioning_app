# Image Captioning Web Application
This project implements a real-time Image Captioning system using a Vision Transformer (ViT) as encoder and GPT-2 as decoder. The system generates natural language captions for images captured via webcam, wrapped in a user-friendly web interface powered by Flask.
## Features
- Capture images directly from webcam and send them to the captioning model via a RESTful API.
- Transformer-based encoder-decoder architecture for high-quality caption generation.
- Pretrained weights used for both ViT and GPT-2 to enhance performance and reduce training cost.
- Real-time caption rendering on the frontend after prediction.
- TTS (Text-To-Speech) support for audio output of generated captions.
## Model Architecture
The model is a Vision-Language Transformer that consists of:
**üî∑Encoder ‚Äì Vision Transformer (ViT)**
- The image is divided into patches (e.g., 16√ó16), each of which is linearly embedded.
- A [CLS] token is prepended, and positional embeddings are added.
- The patch embeddings pass through 12 ViT Encoder Blocks, each consisting of:
  - Multi-Head Self-Attention
  - MLP (Feed Forward)
  - Layer Normalization and Residual Connections

The output [CLS] embedding acts as the visual representation for the decoder.

**üî∂ Decoder ‚Äì GPT-2 Language Model**
- The decoder is based on 12-layer GPT-2 Transformer blocks.
- The input caption tokens are embedded and added with positional encoding.
- The GPT-2 block performs:
  - Causal Multi-Head Self-Attention
  - Cross-Attention with ViT encoder output
  - MLP, Layer Normalization
 
**‚û°Ô∏è Output**
- The final output logits are generated by a linear layer (LM Head) on top of the decoder's output.
- The model is trained to generate the correct caption conditioned on the visual representation.
## Dataset
- MS COCO 2014 dataset was used for training and evaluation.
- Image-caption pairs were preprocessed using:
  - Tokenization with GPT2TokenizerFast
  - Image resizing and normalization compatible with ViT
 
## Training & Results
Trained for 10 epochs with:
- Optimizer: AdamW
- Loss: Cross-Entropy Loss
- Batch size: 32
- Learning rate: 5e-5
## Web Interface
## Run Locally
## File Structure
